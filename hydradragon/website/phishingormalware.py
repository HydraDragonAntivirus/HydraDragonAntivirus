#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import csv
import re
import ipaddress
import argparse
from pathlib import Path
from typing import Set, List, Dict, Tuple, Optional, Any, Union
from urllib.parse import urlparse
from collections import defaultdict
from datetime import datetime

# --- Constants and Configuration ---

# Define the priority order for categories, from highest to lowest.
# This order is used for deduplication.
CATEGORY_PRIORITY = [
    "Mining",
    "Spam",
    "Abuse",
    "BruteForce",
    "DDoS",
    "MaliciousMail",
    "Phishing",
    "Malware",
]

# Define base patterns for different types of lists.
WHITELIST_PATTERNS = ["BenignMail", "WhiteList"]
# We still need all potential category names for matching.
ALL_CATEGORIES = CATEGORY_PRIORITY + WHITELIST_PATTERNS


# --- Core Utility and Validation Functions ---

def get_category_from_filename(filename: str, categories: List[str]) -> Optional[str]:
    """
    Finds the best matching category for a filename by looking for the longest
    matching category string. This correctly handles overlapping names like
    'Malware' and 'MaliciousMail' regardless of their order in the list.
    """
    matches = [cat for cat in categories if cat in filename]
    if not matches:
        return None
    return max(matches, key=len)

def get_base_type(entry_type: str) -> str:
    """
    Returns the base type for deduplication purposes. This allows custom types
    like 'ipv4_phishing_active' to be deduplicated against the base 'ipv4' type.
    """
    if entry_type.startswith('ipv4'):
        return 'ipv4'
    if entry_type.startswith('ipv6'):
        return 'ipv6'
    return entry_type

def get_ip_type(ip_string: str) -> Optional[str]:
    """Validates IP/CIDR syntax and returns 'ipv4' or 'ipv6'."""
    try:
        net = ipaddress.ip_network(ip_string, strict=False)
        return "ipv4" if net.version == 4 else "ipv6"
    except ValueError:
        return None

def is_valid_ip(ip_string: str) -> Optional[str]:
    """
    Validates if a string is a valid public IP address (IPv4/IPv6) or CIDR.
    Excludes private, reserved, and other special-use ranges.
    """
    try:
        net = ipaddress.ip_network(ip_string, strict=False)
        if not net.is_global:
            return None
        return "ipv4" if net.version == 4 else "ipv6"
    except ValueError:
        try:
            ip = ipaddress.ip_address(ip_string)
            if not ip.is_global:
                return None
            return "ipv4" if ip.version == 4 else "ipv6"
        except ValueError:
            return None

def clean_domain(domain: str) -> str:
    """
    Cleans and normalizes a domain string.
    """
    if not isinstance(domain, str):
        return ""
    domain = domain.strip().lower()
    if '://' in domain:
        domain = urlparse(domain).netloc
    domain = domain.split('/')[0].split('?')[0].split(':')[0].strip('.')
    if domain.startswith('www.'):
        domain = domain[4:]
    return domain

def validate_domain(domain: str) -> Tuple[bool, str]:
    """
    Performs a comprehensive validation of a domain name's syntax.
    """
    if not domain:
        return False, "Domain is empty"
    if len(domain) > 253:
        return False, "Domain exceeds 253 characters"
    if not re.match(r'^[a-z0-9\-\.]+$', domain):
        return False, "Domain contains invalid characters"

    labels = domain.split('.')
    if len(labels) < 2:
        return False, "Domain must have at least a TLD (e.g., 'example.com')"

    for label in labels:
        if not label or len(label) > 63:
            return False, f"Invalid label length in '{label}'"
        if label.startswith('-') or label.endswith('-'):
            return False, f"Label '{label}' cannot start or end with a hyphen"
    return True, "Valid domain"

def get_main_domain(domain: str, public_suffixes: Set[str]) -> str:
    """
    Extracts the registrable (main) domain from a full domain string.
    """
    if not domain:
        return ""
    parts = domain.split('.')
    for i in range(len(parts)):
        suffix = '.'.join(parts[i:])
        if suffix in public_suffixes:
            main_domain_part_index = i - 1
            if main_domain_part_index < 0:
                return domain
            return '.'.join(parts[main_domain_part_index:])
    return '.'.join(parts[-2:]) if len(parts) >= 2 else domain

def is_subdomain(domain: str, public_suffixes: Set[str]) -> bool:
    """
    Determines if a domain is a subdomain based on the public suffix list.
    """
    main_domain = get_main_domain(domain, public_suffixes)
    return domain != main_domain

def classify_entry(entry: str, public_suffixes: Set[str], for_whitelist: bool = False) -> Tuple[Optional[str], str, Optional[str]]:
    """
    Classifies a string as an IP, domain, subdomain, or invalid.
    """
    cleaned_entry = entry.strip()

    ip_check_func = get_ip_type if for_whitelist else is_valid_ip
    ip_type = ip_check_func(cleaned_entry)
    if ip_type:
        return ip_type, cleaned_entry, None

    cleaned_domain = clean_domain(cleaned_entry)
    is_valid, error_msg = validate_domain(cleaned_domain)

    if not is_valid:
        if get_ip_type(cleaned_entry):
             return "non_public_ip" if not for_whitelist else get_ip_type(cleaned_entry), cleaned_entry, None
        return None, entry, error_msg

    if is_subdomain(cleaned_domain, public_suffixes):
        return "subdomain", cleaned_domain, None
    else:
        return "domain", cleaned_domain, None

# --- File I/O and Data Handling Functions ---

def read_csv_file(file_path: Path) -> List[Tuple[str, str]]:
    """
    Reads a CSV file and returns a list of (entry, reference) tuples.
    """
    entries = []
    try:
        with file_path.open('r', encoding='utf-8', newline='') as f:
            try:
                has_header = csv.Sniffer().has_header(f.read(1024))
            except csv.Error:
                has_header = False
            f.seek(0)
            reader = csv.reader(f)
            if has_header:
                next(reader, None)

            for row in reader:
                if not row:
                    continue
                entry = row[0].strip()
                if entry and not entry.startswith('#'):
                    reference = row[1].strip() if len(row) > 1 else "Unknown"
                    entries.append((entry, reference))
    except FileNotFoundError:
        print(f"Info: File not found, skipping: {file_path.name}")
    except Exception as e:
        print(f"Error reading {file_path.name}: {e}")
    return entries

def write_csv_output(filename: Path, data: Set[Tuple[str, str]], header: List[str]) -> None:
    """
    Writes a set of data to a CSV file with robust sorting.
    """
    try:
        def unified_sort_key(item_tuple: Tuple[str, str]):
            """Creates a key for sorting IPs and domains together."""
            item_str = item_tuple[0]
            try:
                net = ipaddress.ip_network(item_str, strict=False)
                return (0, net.version, net.network_address)
            except ValueError:
                return (1, item_str)

        sorted_data = sorted(list(data), key=unified_sort_key)

        with filename.open('w', encoding='utf-8', newline='') as f:
            writer = csv.writer(f)
            writer.writerow(header)
            writer.writerows(sorted_data)
        print(f"Created {filename.name} with {len(sorted_data)} entries.")
    except Exception as e:
        print(f"Error writing to {filename.name}: {e}")


def prompt_for_category_selection() -> Optional[str]:
    """
    Displays a menu of categories and prompts the user to select one.
    """
    print("\nWhich category should the new entries be added to?")
    for i, cat in enumerate(ALL_CATEGORIES, 1):
        print(f"  {i}. {cat}")

    try:
        choice = input(f"Select a category (1-{len(ALL_CATEGORIES)}): ").strip()
        choice_num = int(choice)
        if 1 <= choice_num <= len(ALL_CATEGORIES):
            return ALL_CATEGORIES[choice_num - 1]
        else:
            print("Invalid selection. Aborting.")
            return None
    except (ValueError, IndexError):
        print("Invalid input. Please enter a number from the list. Aborting.")
        return None

# --- Main Processing Logic ---

def process_lists(input_dir: str, output_dir: str, new_list_path: Optional[str]) -> None:
    """
    The main function to orchestrate the entire list processing workflow.
    """
    input_path = Path(input_dir).resolve()
    output_path = Path(output_dir).resolve()
    output_path.mkdir(exist_ok=True)

    print(f"--- Unified IP and Domain Processor ---")
    print(f"Input Directory: {input_path}")
    print(f"Output Directory: {output_path}")
    print("-" * 40)

    public_suffixes_file = input_path / "public_suffixes.txt"
    public_suffixes: Set[str] = set()
    if public_suffixes_file.exists():
        with public_suffixes_file.open('r', encoding='utf-8') as f:
            public_suffixes.update(line.strip().lower() for line in f if line.strip() and not line.startswith('#'))
        print(f"Loaded {len(public_suffixes)} public suffixes.")
    else:
        print("Warning: 'public_suffixes.txt' not found. Subdomain detection may be less accurate.")

    blocklist_data = defaultdict(lambda: defaultdict(set))
    whitelist_data = defaultdict(lambda: defaultdict(set))
    invalid_entries: List[Tuple[str, str, str]] = []
    whitelisted_removals: List[Tuple[str, str, str]] = []
    all_input_files = list(input_path.glob('*.csv'))

    # NEW: A dedicated set to store the main domains from BenignMail lists.
    # This ensures that a record like 'mail.google.com' will cause 'google.com' to be fully whitelisted.
    benign_main_domains = set()

    print("\n--- Step 1: Processing Whitelists ---")
    whitelist_files = [f for f in all_input_files if get_category_from_filename(f.name, WHITELIST_PATTERNS)]
    for file_path in whitelist_files:
        print(f"Reading whitelist: {file_path.name}")
        file_key = file_path.name
        # NEW: Check if the file is a BenignMail list.
        is_benign_mail_file = "BenignMail" in file_path.name
        for entry, reference in read_csv_file(file_path):
            entry_type, cleaned_entry, err = classify_entry(entry, public_suffixes, for_whitelist=True)

            # NEW: If the entry is from a BenignMail list, add its main domain to the dedicated set.
            if is_benign_mail_file and entry_type in ['domain', 'subdomain']:
                main_domain = get_main_domain(cleaned_entry, public_suffixes)
                if main_domain:
                    benign_main_domains.add(main_domain)

            if entry_type in ['ipv4', 'ipv6', 'domain', 'subdomain']:
                whitelist_data[file_key][entry_type].add((cleaned_entry, reference))
            else:
                invalid_entries.append((entry, err or "Invalid format for whitelist", file_path.name))

    whitelisted_domains = set()
    whitelisted_subdomains = set()
    whitelisted_single_ips = set()
    whitelisted_networks = []
    for file_data in whitelist_data.values():
        whitelisted_domains.update(dom for dom, ref in file_data.get('domain', set()))
        whitelisted_subdomains.update(sub for sub, ref in file_data.get('subdomain', set()))
        all_ips = file_data.get('ipv4', set()) | file_data.get('ipv6', set())
        for ip_str, ref in all_ips:
            try:
                net = ipaddress.ip_network(ip_str, strict=False)
                if net.num_addresses == 1:
                    whitelisted_single_ips.add(net.network_address)
                else:
                    whitelisted_networks.append(net)
            except ValueError:
                invalid_entries.append((ip_str, "Invalid IP/CIDR in whitelist", "Whitelist Processing"))

    # NEW: Add the main domains from BenignMail to the global whitelist.
    # This ensures that legitimate mail services (and all their subdomains) do not appear on any blocklist.
    original_domain_count = len(whitelisted_domains)
    whitelisted_domains.update(benign_main_domains)
    added_count = len(whitelisted_domains) - original_domain_count
    if added_count > 0:
        print(f"Promoted {added_count} main domains from BenignMail lists to the global whitelist.")


    print(f"Found {len(whitelisted_single_ips)} whitelisted single IPs, "
          f"{len(whitelisted_networks)} whitelisted networks, "
          f"{len(whitelisted_domains)} domains, "
          f"{len(whitelisted_subdomains)} subdomains for filtering.")

    print("\n--- Step 2: Processing Blocklists ---")
    blocklist_files = [f for f in all_input_files if get_category_from_filename(f.name, CATEGORY_PRIORITY)]
    for file_path in blocklist_files:
        category = get_category_from_filename(file_path.name, CATEGORY_PRIORITY)
        if not category:
            continue
        
        print(f"Reading blocklist: {file_path.name} (Category: {category})")
        for entry, reference in read_csv_file(file_path):
            entry_type, cleaned_entry, error_msg = classify_entry(entry, public_suffixes)

            if not entry_type or entry_type == "non_public_ip":
                invalid_entries.append((entry, error_msg or "Non-public or invalid format", file_path.name))
                continue

            # UPDATED: Determine if a Phishing IPv4 is active or inactive based on the CSV FILENAME
            final_entry_type = entry_type
            if category == 'Phishing' and entry_type == 'ipv4':
                if 'InActive' in file_path:    
                    final_entry_type = 'ipv4_phishing_inactive'
                else:
                    final_entry_type = 'ipv4_phishing_active'

            # Note: The whitelist check logic already includes main domains from BenignMail.
            # The check is performed against the consolidated global whitelists.
            is_whitelisted = False
            if entry_type in ['ipv4', 'ipv6']:
                try:
                    net = ipaddress.ip_network(cleaned_entry, strict=False)
                    if net.num_addresses == 1:
                        addr = net.network_address
                        if addr in whitelisted_single_ips or any(addr in w_net for w_net in whitelisted_networks):
                            is_whitelisted = True
                    elif any(net.overlaps(w_net) for w_net in whitelisted_networks):
                        is_whitelisted = True
                except ValueError: pass
            elif entry_type == 'subdomain':
                main_domain = get_main_domain(cleaned_entry, public_suffixes)
                if cleaned_entry in whitelisted_subdomains or main_domain in whitelisted_domains:
                    is_whitelisted = True
            elif entry_type == 'domain':
                if cleaned_entry in whitelisted_domains:
                    is_whitelisted = True

            if is_whitelisted:
                whitelisted_removals.append((cleaned_entry, reference, file_path.name))
            else:
                blocklist_data[category][final_entry_type].add((cleaned_entry, reference))

    if new_list_path:
        print("\n--- Step 3: Processing New Entries ---")
        new_list_file = Path(new_list_path)
        if new_list_file.exists():
            target_category = prompt_for_category_selection()
            if target_category:
                new_entries = read_csv_file(new_list_file)
                additional_ref = input("Enter a reference to add to all new entries: ").strip()
                added_count = 0
                is_target_whitelist = target_category in WHITELIST_PATTERNS

                for entry, existing_ref in new_entries:
                    # Construct reference properly
                    if additional_ref:
                        if existing_ref and existing_ref != "Unknown":
                            final_ref = f"{existing_ref} + {additional_ref}"
                        else:
                            final_ref = additional_ref
                    else:
                        final_ref = existing_ref

                    entry_type, cleaned_entry, error_msg = classify_entry(entry, public_suffixes, for_whitelist=is_target_whitelist)
                    
                    if entry_type in ['ipv4', 'ipv6', 'domain', 'subdomain']:
                        if is_target_whitelist:
                            target_filename = f"{target_category}.csv"
                            whitelist_data[target_filename][entry_type].add((cleaned_entry, final_ref))
                            added_count += 1
                        else:
                            # Check if entry is whitelisted
                            is_whitelisted = False
                            if entry_type in ['ipv4', 'ipv6']:
                                try:
                                    net = ipaddress.ip_network(cleaned_entry, strict=False)
                                    if net.num_addresses == 1:
                                        addr = net.network_address
                                        if addr in whitelisted_single_ips or any(addr in w_net for w_net in whitelisted_networks): 
                                            is_whitelisted = True
                                    elif any(net.overlaps(w_net) for w_net in whitelisted_networks): 
                                        is_whitelisted = True
                                except ValueError: 
                                    pass
                            elif entry_type == 'subdomain':
                                main_domain = get_main_domain(cleaned_entry, public_suffixes)
                                if cleaned_entry in whitelisted_subdomains or main_domain in whitelisted_domains: 
                                    is_whitelisted = True
                            elif entry_type == 'domain':
                                if cleaned_entry in whitelisted_domains: 
                                    is_whitelisted = True
                            
                            if is_whitelisted:
                                whitelisted_removals.append((cleaned_entry, final_ref, new_list_file.name))
                            else:
                                # UPDATED: Apply active/inactive logic to new entries based on FILENAME
                                final_entry_type = entry_type
                                if target_category == 'Phishing' and entry_type == 'ipv4':
                                    if 'inactive' in file_path.name.lower(): 
                                        final_entry_type = 'ipv4_phishing_inactive'
                                    else:
                                        final_entry_type = 'ipv4_phishing_active'
                                
                                blocklist_data[target_category][final_entry_type].add((cleaned_entry, final_ref))
                                added_count += 1
                    else:
                        invalid_entries.append((entry, error_msg or "Invalid format", new_list_file.name))
                
                print(f"Processed {added_count} new entries for category '{target_category}'.")
            else:
                print("No category selected. Skipping new entries processing.")
        else:
            print(f"Error: --new-list file not found: {new_list_path}")

    print("\n--- Step 4: Applying Priority Deduplication ---")
    seen_entries = defaultdict(set)
    final_data = defaultdict(lambda: defaultdict(dict))  # Changed to dict to merge references
    
    for category in CATEGORY_PRIORITY:
        if category not in blocklist_data:
            continue
        sorted_entry_types = sorted(blocklist_data[category].keys())
        for entry_type in sorted_entry_types:
            # First, merge references for duplicate entries within the same category/type
            merged_entries = defaultdict(set)
            for entry, reference in blocklist_data[category][entry_type]:
                merged_entries[entry].add(reference)
            
            # Now apply priority deduplication
            for entry, references in merged_entries.items():
                base_type = get_base_type(entry_type)
                if entry not in seen_entries[base_type]:
                    # Merge all references for this entry, removing duplicates and "Unknown" when better refs exist
                    ref_set = set(ref for ref in references if ref)
                    if "Unknown" in ref_set and len(ref_set) > 1:
                        ref_set.discard("Unknown")
                    
                    final_reference = " | ".join(sorted(ref_set)) if ref_set else "Unknown"
                    final_data[category][entry_type][entry] = final_reference
                    seen_entries[base_type].add(entry)

    print("\n--- Step 5: Writing Output Files ---")
    output_filename_map = {
        ('Phishing', 'ipv4_phishing_active'): 'IPv4PhishingActive.csv',
        ('Phishing', 'ipv4_phishing_inactive'): 'IPv4PhisingInActive.csv',
        ('Phishing', 'domain'): 'PhishingDomains.csv',
        ('Phishing', 'subdomain'): 'PhishingSubDomains.csv',
        ('Malware', 'ipv4'): 'IPv4Malware.csv',
        ('Malware', 'ipv6'): 'IPv6Malware.csv',
        ('Malware', 'domain'): 'MalwareDomains.csv',
        ('Malware', 'subdomain'): 'MalwareSubDomains.csv',
        ('MaliciousMail', 'ipv4'): 'IPv4MaliciousMail.csv',
        ('MaliciousMail', 'ipv6'): 'IPv6MaliciousMail.csv',
        ('MaliciousMail', 'domain'): 'MaliciousMailDomains.csv',
        ('MaliciousMail', 'subdomain'): 'MaliciousMailSubDomains.csv',
        ('DDoS', 'ipv4'): 'IPv4DDoS.csv',
        ('DDoS', 'ipv6'): 'IPv6DDoS.csv',
        ('BruteForce', 'ipv4'): 'IPv4BruteForce.csv',
        ('BruteForce', 'ipv6'): 'IPv6BruteForce.csv',
        ('Abuse', 'ipv4'): 'IPv4Abuse.csv',
        ('Abuse', 'ipv6'): 'IPv6Abuse.csv',
        ('Abuse', 'domain'): 'AbuseDomains.csv',
        ('Abuse', 'subdomain'): 'AbuseSubDomains.csv',
        ('Spam', 'ipv4'): 'IPv4Spam.csv',
        ('Spam', 'ipv6'): 'IPv6Spam.csv',
        ('Spam', 'domain'): 'SpamDomains.csv',
        ('Spam', 'subdomain'): 'SpamSubDomains.csv',
        ('Mining', 'ipv4'): 'IPv4Mining.csv',
        ('Mining', 'ipv6'): 'IPv6Mining.csv',
        ('Mining', 'domain'): 'MiningDomains.csv',
        ('Mining', 'subdomain'): 'MiningSubDomains.csv',
    }

    for category, types_data in final_data.items():
        for entry_type, entries_dict in types_data.items():
            filename_key = (category, entry_type)
            if filename_key in output_filename_map:
                filename = output_filename_map[filename_key]
                output_file = output_path / filename
                # Convert dict back to set of tuples for write_csv_output
                data_set = {(entry, reference) for entry, reference in entries_dict.items()}
                write_csv_output(output_file, data_set, ['entry', 'reference'])
            else:
                print(f"Warning: No output mapping found for {category} - {entry_type} (found {len(entries_dict)} entries)")
                # Create a generic filename for unmapped types
                generic_filename = f"{entry_type.title()}{category}.csv"
                output_file = output_path / generic_filename
                data_set = {(entry, reference) for entry, reference in entries_dict.items()}
                write_csv_output(output_file, data_set, ['entry', 'reference'])

    print("\n--- Writing Cleaned Whitelist Files by Type ---")
    
    # Define whitelist output filename mapping
    whitelist_output_map = {
        'domain': 'WhiteListDomains.csv',
        'subdomain': 'WhiteListSubDomains.csv', 
        'ipv4': 'WhiteListIPv4.csv',
        'ipv6': 'WhiteListIPv6.csv'
    }
    
    # Aggregate all whitelist data by type across all whitelist files and merge references
    aggregated_whitelist = defaultdict(lambda: defaultdict(set))
    
    for filename, file_content_by_type in whitelist_data.items():
        for entry_type, data_set in file_content_by_type.items():
            if entry_type in whitelist_output_map:
                for entry, reference in data_set:
                    aggregated_whitelist[entry_type][entry].add(reference)
    
    # Write separate files for each whitelist type
    for entry_type, output_filename in whitelist_output_map.items():
        if entry_type in aggregated_whitelist and aggregated_whitelist[entry_type]:
            # Merge references for duplicate entries
            final_whitelist_data = set()
            for entry, references in aggregated_whitelist[entry_type].items():
                ref_set = set(ref for ref in references if ref)
                if "Unknown" in ref_set and len(ref_set) > 1:
                    ref_set.discard("Unknown")
                
                final_reference = " | ".join(sorted(ref_set)) if ref_set else "Unknown"
                final_whitelist_data.add((entry, final_reference))
            
            output_file = output_path / output_filename
            write_csv_output(output_file, final_whitelist_data, ['entry', 'reference'])
        else:
            print(f"No {entry_type} entries found for whitelist output")

    print("\n--- Step 6: Generating Reports ---")
    if invalid_entries:
        report_file = output_path / "report_invalid_entries.txt"
        with report_file.open('w', encoding='utf-8') as f:
            f.write(f"Invalid Entries Report - {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
            f.write("="*60 + "\n")
            f.write("The following entries were ignored due to validation errors.\n\n")
            sorted_invalids = sorted(invalid_entries, key=lambda x: x[2])
            for entry, reason, source in sorted_invalids:
                f.write(f"- Source: {source}\n")
                f.write(f"  Entry : '{entry}'\n")
                f.write(f"  Reason: {reason}\n\n")
        print(f"Warning: Found {len(invalid_entries)} invalid entries. See {report_file.name} for details.")

    if whitelisted_removals:
        report_file = output_path / "report_whitelisted_removals.txt"
        with report_file.open('w', encoding='utf-8') as f:
            f.write(f"Whitelisted Removals Report - {datetime.now().strftime('%Y-%m-%d %H:%M')}\n")
            f.write("="*60 + "\n")
            f.write("The following entries were removed from blocklists because they are on a whitelist.\n\n")
            sorted_removals = sorted(whitelisted_removals, key=lambda x: x[2])
            for entry, ref, source in sorted_removals:
                f.write(f"- Source     : {source}\n")
                f.write(f"  Entry      : {entry}\n")
                f.write(f"  Reference  : {ref}\n\n")
        print(f"Info: Removed {len(whitelisted_removals)} whitelisted entries. See {report_file.name} for details.")

    print("\n--- Writing BenignMail Whitelist Files ---")

    benign_output_map = {
        'domain': 'BenignMailDomains.csv',
        'subdomain': 'BenignMailSubDomains.csv',
        'ipv4': 'BenignMailIPv4.csv',
        'ipv6': 'BenignMailIPv6.csv'
    }

    benign_whitelist_aggregated = defaultdict(lambda: defaultdict(set))

    for filename, file_content_by_type in whitelist_data.items():
        if "BenignMail" in filename:
            for entry_type, data_set in file_content_by_type.items():
                if entry_type in benign_output_map:
                    for entry, reference in data_set:
                        benign_whitelist_aggregated[entry_type][entry].add(reference)

    for entry_type, output_filename in benign_output_map.items():
        if benign_whitelist_aggregated[entry_type]:
            final_benign_data = set()
            for entry, references in benign_whitelist_aggregated[entry_type].items():
                ref_set = set(ref for ref in references if ref)
                if "Unknown" in ref_set and len(ref_set) > 1:
                    ref_set.discard("Unknown")
                final_reference = " | ".join(sorted(ref_set)) if ref_set else "Unknown"
                final_benign_data.add((entry, final_reference))

            output_file = output_path / output_filename
            write_csv_output(output_file, final_benign_data, ['entry', 'reference'])
        else:
            print(f"No {entry_type} entries found for BenignMail output.")

    print("\nProcessing complete.")

# --- Main Execution Block ---

if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Unified IP and Domain List Processor. Cleans, validates, and deduplicates security intelligence lists from CSV files.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument(
        '--input-dir',
        default=".",
        help="Directory containing your source CSV files (default: current directory)."
    )
    parser.add_argument(
        '--output-dir',
        default="cleaned",
        help="Directory where cleaned files and reports will be saved (default: 'cleaned')."
    )
    parser.add_argument(
        '--new-list',
        type=str,
        help="Path to a CSV or text file with new entries to add.\n"
             "You will be prompted to choose a category for these new entries."
    )
    args = parser.parse_args()
    try:
        process_lists(args.input_dir, args.output_dir, args.new_list)
    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        print("Please check your file permissions and formats.")
