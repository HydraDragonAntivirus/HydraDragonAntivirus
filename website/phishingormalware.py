import os
from pathlib import Path
from typing import Dict, Set, List, Tuple
import re
from urllib.parse import urlparse
from collections import defaultdict
import argparse

def validate_domain(domain: str) -> Tuple[bool, str]:
    """
    Enhanced domain validation with detailed checking.
    
    Args:
        domain: Domain name to validate
    
    Returns:
        Tuple[bool, str]: (is_valid, error_message)
    """
    if not domain:
        return False, "Empty domain"
    
    # Remove any whitespace and convert to lowercase
    domain = domain.strip().lower()
    
    # Basic length check
    if len(domain) > 253:
        return False, "Domain too long (max 253 characters)"
    
    # Check for invalid characters
    allowed_chars = re.compile(r'^[a-z0-9\-\.]+$')
    if not allowed_chars.match(domain):
        return False, "Contains invalid characters"
    
    # Split into labels
    labels = domain.split('.')
    
    # Must have at least two labels
    if len(labels) < 2:
        return False, "Missing TLD"
    
    # Check each label
    for label in labels:
        # Length check for each label
        if not label or len(label) > 63:
            return False, "Label empty or too long (max 63 characters)"
        
        # Must not start or end with hyphen
        if label.startswith('-') or label.endswith('-'):
            return False, "Label cannot start or end with hyphen"
        
        # Check for double hyphens in non-IDNA
        if '--' in label and not label.startswith('xn--'):
            return False, "Double hyphens only allowed in IDNA labels"
    
    return True, "Valid domain"

def clean_domain(domain: str) -> str:
    """
    Clean and normalize a domain name.
    
    Args:
        domain: Raw domain string
    
    Returns:
        str: Cleaned domain name
    """
    # Remove any whitespace
    domain = domain.strip()
    
    # Convert to lowercase
    domain = domain.lower()
    
    # Remove any protocol prefix
    if '://' in domain:
        domain = urlparse(domain).netloc
    
    # Remove any paths or query parameters
    domain = domain.split('/')[0]
    domain = domain.split('?')[0]
    
    # Remove any ports
    domain = domain.split(':')[0]
    
    # Remove any leading/trailing dots
    domain = domain.strip('.')
    
    return domain

def process_domain_files(input_dir: str = ".", output_dir: str = "cleaned") -> None:
    """
    Process domain files by removing duplicates and whitelisted domains.
    
    Args:
        input_dir: Directory containing the domain files (default: current directory)
        output_dir: Directory where cleaned files will be saved (default: 'cleaned')
    """
    input_path = Path(input_dir).resolve()
    output_path = Path(output_dir).resolve()
    
    print(f"Input directory: {input_path}")
    print(f"Output directory: {output_path}")
    
    # Define file names with priorities
    files_priority = [
        "SpamDomains.txt",
        "MiningDomains.txt",
        "AbuseDomains.txt",
        "PhishingDomains.txt",
        "MalwareDomainsMail.txt",
        "MalwareDomains.txt"
    ]
    
    whitelist_files = [
        "WhiteListDomainsMail.txt",
        "WhiteListDomains.txt"
    ]
    
    # Print files we're looking for
    print("\nLooking for these files:")
    for file_name in files_priority + whitelist_files:
        file_path = input_path / file_name
        print(f"- {file_name}: {'Found' if file_path.exists() else 'Not found'}")
    
    # Create output directory if it doesn't exist
    output_path.mkdir(exist_ok=True)
    print(f"\nCreated output directory: {output_path}")
    
    # Dictionary to store domains and their validation status
    domain_sets: Dict[str, Set[str]] = {}
    invalid_domains: Dict[str, List[Tuple[str, str]]] = defaultdict(list)
    
    # Process whitelist domains
    whitelist_domains: Set[str] = set()
    for whitelist_file in whitelist_files:
        try:
            file_path = input_path / whitelist_file
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    domain = line.strip()
                    if not domain or domain.startswith('#'):
                        continue
                    
                    cleaned_domain = clean_domain(domain)
                    is_valid, error_msg = validate_domain(cleaned_domain)
                    
                    if is_valid:
                        whitelist_domains.add(cleaned_domain)
                    else:
                        invalid_domains[whitelist_file].append((domain, error_msg))
        except FileNotFoundError:
            print(f"Warning: {whitelist_file} not found. Proceeding without it.")
    
    # Read all domain files
    for file_name in files_priority:
        try:
            file_path = input_path / file_name
            valid_domains = set()
            
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    domain = line.strip()
                    if not domain or domain.startswith('#'):
                        continue
                    
                    cleaned_domain = clean_domain(domain)
                    is_valid, error_msg = validate_domain(cleaned_domain)
                    
                    if is_valid:
                        if cleaned_domain not in whitelist_domains:
                            valid_domains.add(cleaned_domain)
                    else:
                        invalid_domains[file_name].append((domain, error_msg))
            
            domain_sets[file_name] = valid_domains
            
        except FileNotFoundError:
            print(f"Warning: {file_name} not found. Skipping.")
            domain_sets[file_name] = set()
    
    # Process domains according to priority
    for i, high_priority_file in enumerate(files_priority):
        if high_priority_file not in domain_sets:
            continue
        for low_priority_file in files_priority[i+1:]:
            if low_priority_file not in domain_sets:
                continue
            # Move duplicate domains to higher priority file
            duplicates = domain_sets[high_priority_file] & domain_sets[low_priority_file]
            # Remove duplicates from lower priority file
            domain_sets[low_priority_file] -= duplicates
    
    # Write processed domains back to files
    for file_name, domains in domain_sets.items():
        if domains:  # Only write files that have domains
            output_file = output_path / f"cleaned_{file_name}"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(domains)))
            print(f"Created {output_file} with {len(domains)} unique domains")
    
    # Write whitelist domains to cleaned file
    if whitelist_domains:
        for whitelist_file in whitelist_files:
            output_file = output_path / f"cleaned_{whitelist_file}"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(whitelist_domains)))
            print(f"Created {output_file} with {len(whitelist_domains)} domains")
    
    # Write invalid domains report
    if invalid_domains:
        invalid_file = output_path / "invalid_domains.txt"
        with open(invalid_file, 'w', encoding='utf-8') as f:
            f.write("Invalid Domains Report\n")
            f.write("====================\n\n")
            for file_name, domains in invalid_domains.items():
                f.write(f"\n{file_name}:\n")
                for domain, error in domains:
                    f.write(f"  - {domain}: {error}\n")
        print(f"\nWarning: Found invalid domains. See {invalid_file} for details")

if __name__ == "__main__":    
    parser = argparse.ArgumentParser(description='Process domain files and remove duplicates.')
    parser.add_argument('--input-dir', default=".", help='Directory containing domain files')
    parser.add_argument('--output-dir', default="cleaned", help='Directory for cleaned files')
    
    args = parser.parse_args()
    process_domain_files(args.input_dir, args.output_dir)