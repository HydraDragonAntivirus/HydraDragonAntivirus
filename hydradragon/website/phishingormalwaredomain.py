import os
from pathlib import Path
from typing import Dict, Set, List, Tuple
import re
from urllib.parse import urlparse
from collections import defaultdict
import argparse

def validate_domain(domain: str) -> Tuple[bool, str]:
    """
    Enhanced domain validation with detailed checking.
    
    Args:
        domain: Domain name to validate
    
    Returns:
        Tuple[bool, str]: (is_valid, error_message)
    """
    if not domain:
        return False, "Empty domain"
    
    # Remove any whitespace and convert to lowercase
    domain = domain.strip().lower()
    
    # Basic length check
    if len(domain) > 253:
        return False, "Domain too long (max 253 characters)"
    
    # Check for invalid characters
    allowed_chars = re.compile(r'^[a-z0-9\-\.]+$')
    if not allowed_chars.match(domain):
        return False, "Contains invalid characters"
    
    # Split into labels
    labels = domain.split('.')
    
    # Must have at least two labels
    if len(labels) < 2:
        return False, "Missing TLD"
    
    # Check each label
    for label in labels:
        # Length check for each label
        if not label or len(label) > 63:
            return False, "Label empty or too long (max 63 characters)"
        
        # Must not start or end with hyphen
        if label.startswith('-') or label.endswith('-'):
            return False, "Label cannot start or end with hyphen"
        
        # Check for double hyphens in non-IDNA
        if '--' in label and not label.startswith('xn--'):
            return False, "Double hyphens only allowed in IDNA labels"
    
    return True, "Valid domain"

def clean_domain(domain: str) -> str:
    """
    Clean and normalize a domain name.
    
    Args:
        domain: Raw domain string
    
    Returns:
        str: Cleaned domain name
    """
    # Remove any whitespace
    domain = domain.strip()
    
    # Convert to lowercase
    domain = domain.lower()
    
    # Remove any protocol prefix
    if '://' in domain:
        domain = urlparse(domain).netloc
    
    # Remove any paths or query parameters
    domain = domain.split('/')[0]
    domain = domain.split('?')[0]
    
    # Remove any ports
    domain = domain.split(':')[0]
    
    # Remove any leading/trailing dots
    domain = domain.strip('.')
    
    # Remove 'www.' prefix if it exists
    if domain.startswith('www.'):
        domain = domain[4:]
    
    return domain

def get_main_domain(domain: str, public_suffixes: Set[str]) -> str:
    """
    Extracts the registrable or main domain from a full domain string.
    e.g., 'sub.example.co.uk' -> 'example.co.uk' based on 'co.uk' in suffixes.
    e.g., 'mail.google.com' -> 'google.com' if no match in suffixes.
    
    Args:
        domain: The domain name to process.
        public_suffixes: A set of known public suffixes (like 'co.uk', 'com.tr').
        
    Returns:
        The main part of the domain.
    """
    if not domain:
        return ""
        
    parts = domain.split('.')
    
    # Iterate from longest possible suffix to shortest
    for i in range(len(parts) - 1):
        suffix = '.'.join(parts[i:])
        if suffix in public_suffixes:
            # We found the public suffix. The main domain is the label
            # before it, plus the suffix itself.
            if i > 0:
                main_domain_part_index = i - 1
                return '.'.join(parts[main_domain_part_index:])
            else:
                # This means the domain itself is a public suffix (e.g., 'co.uk').
                # It's not a registrable domain. Return as is.
                return domain

    # If no match was found in our custom list, fall back to the standard
    # assumption: the main domain is the last two parts (e.g., google.com).
    if len(parts) >= 2:
        return '.'.join(parts[-2:])
    else:
        # For single-label domains like 'localhost'.
        return domain

def is_subdomain(domain: str, public_suffixes: Set[str]) -> bool:
    """
    Heuristic to determine if a domain is likely a subdomain.
    It compares the domain with its calculated main domain.
    
    Args:
        domain: Cleaned domain string.
        public_suffixes: A set of known public suffixes.
    
    Returns:
        bool: True if it is likely a subdomain, False otherwise.
    """
    # A domain is a subdomain if it's not identical to its main domain.
    # e.g., 'sub.example.co.uk' != 'example.co.uk' -> True
    # e.g., 'example.co.uk' == 'example.co.uk' -> False
    return domain != get_main_domain(domain, public_suffixes)

def process_domain_files(input_dir: str = ".", output_dir: str = "cleaned") -> None:
    """
    Process domain and subdomain files by removing duplicates and whitelisted domains.
    Also, for whitelist files, if a subdomain file is missing but the corresponding
    domain whitelist exists and contains subdomain entries, the missing file is derived.
    
    Args:
        input_dir: Directory containing the domain files (default: current directory)
        output_dir: Directory where cleaned files will be saved (default: 'cleaned')
    """
    input_path = Path(input_dir).resolve()
    output_path = Path(output_dir).resolve()
    
    print(f"Input directory: {input_path}")
    print(f"Output directory: {output_path}")
    
    # --- Load Public Suffix List ---
    public_suffixes_file = input_path / "public_suffixes.txt"
    public_suffixes: Set[str] = set()
    if public_suffixes_file.exists():
        print(f"\nFound public suffix list: {public_suffixes_file}")
        with open(public_suffixes_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip().lower()
                if line and not line.startswith('#'):
                    public_suffixes.add(line)
        print(f"Loaded {len(public_suffixes)} public suffixes.")
    else:
        print("\nWarning: 'public_suffixes.txt' not found. Using standard TLD rule (e.g., .com, .net).")
        print("Create this file and add entries like 'co.uk' or 'com.tr' for more accurate subdomain handling.")

    # Define file names with priorities for regular domain files
    files_priority = [
        "SpamDomains.txt",
        "MiningDomains.txt",
        "AbuseDomains.txt",
        "PhishingDomains.txt",
        "MalwareDomainsMail.txt",
        "MalwareDomains.txt"
    ]
    
    # Define file names with priorities for regular subdomain files
    subdomains_priority = [
        "SpamSubDomains.txt",
        "MiningSubDomains.txt",
        "AbuseSubDomains.txt",
        "PhishingSubDomains.txt",
        "MalwareSubDomainsMail.txt",
        "MalwareSubDomains.txt"
    ]
    
    # Whitelist files (the input names we expect)
    whitelist_domain_files = [
        "WhiteListDomainsMail.txt",
        "WhiteListDomains.txt"
    ]
    whitelist_subdomain_files = [
        "WhiteListSubDomainsMail.txt",
        "WhiteListSubDomains.txt"
    ]
    # For initial reading we consider all whitelist files together.
    whitelist_files = whitelist_domain_files + whitelist_subdomain_files
    
    # Display which files are present in the input directory
    print("\nLooking for these files:")
    all_files_to_check = files_priority + subdomains_priority + whitelist_files + ["public_suffixes.txt"]
    for file_name in all_files_to_check:
        file_path = input_path / file_name
        print(f"- {file_name}: {'Found' if file_path.exists() else 'Not found'}")
    
    # Create output directory if it doesn't exist
    output_path.mkdir(exist_ok=True)
    print(f"\nCreated output directory: {output_path}")
    
    # Dictionaries to store processed content and invalid entries.
    domain_sets: Dict[str, Set[str]] = {}
    subdomain_sets: Dict[str, Set[str]] = {}
    invalid_domains: Dict[str, List[Tuple[str, str]]] = defaultdict(list)
    invalid_subdomains: Dict[str, List[Tuple[str, str]]] = defaultdict(list)
    
    # --- Process whitelist files from input (if they exist) ---
    # Build a dictionary mapping whitelist file name -> set of cleaned entries.
    whitelist_inputs: Dict[str, Set[str]] = {}
    for wf in whitelist_files:
        wf_path = input_path / wf
        if wf_path.exists():
            content = set()
            with open(wf_path, 'r', encoding='utf-8') as f:
                for line in f:
                    line = line.strip()
                    if not line or line.startswith('#'):
                        continue
                    cleaned = clean_domain(line)
                    valid, error_msg = validate_domain(cleaned)
                    if valid:
                        content.add(cleaned)
                    else:
                        # Log invalid entries if desired.
                        if wf.lower().startswith("whitelistsub"):
                            invalid_subdomains[wf].append((line, error_msg))
                        else:
                            invalid_domains[wf].append((line, error_msg))
            whitelist_inputs[wf] = content
        else:
            print(f"Warning: {wf} not found in input.")
    
    # --- Derive whitelist subdomain files if they are missing ---
    # For each expected whitelist subdomain file, if it does not exist,
    # try to derive it from the corresponding domain whitelist file.
    derived_whitelist_subdomains: Dict[str, Set[str]] = {}
    for sub_file in whitelist_subdomain_files:
        if sub_file not in whitelist_inputs:
            # Map the subdomain file to its corresponding domain whitelist file.
            if "mail" in sub_file.lower():
                domain_file = "WhiteListDomainsMail.txt"
            else:
                domain_file = "WhiteListDomains.txt"
            if domain_file in whitelist_inputs:
                # Filter entries that are likely subdomains using the new logic.
                derived = {entry for entry in whitelist_inputs[domain_file] if is_subdomain(entry, public_suffixes)}
                if derived:
                    derived_whitelist_subdomains[sub_file] = derived
        else:
            derived_whitelist_subdomains[sub_file] = whitelist_inputs[sub_file]
    
    # For whitelist domain files, simply use the ones read from input.
    whitelist_domain_outputs: Dict[str, Set[str]] = {}
    for dom_file in whitelist_domain_files:
        if dom_file in whitelist_inputs:
            whitelist_domain_outputs[dom_file] = whitelist_inputs[dom_file]
    
    # --- Process non-whitelist domain files ---
    for file_name in files_priority:
        file_path = input_path / file_name
        valid_domains = set()
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    domain = line.strip()
                    if not domain or domain.startswith('#'):
                        continue
                    cleaned_domain_str = clean_domain(domain)
                    valid_status, error_msg = validate_domain(cleaned_domain_str)
                    if valid_status:
                        # Get the main domain to check against domain whitelists.
                        # This ensures that if a subdomain is in a "Domains" file,
                        # it's still correctly checked against the main domain whitelist.
                        main_domain = get_main_domain(cleaned_domain_str, public_suffixes)
                        
                        # An entry is kept if:
                        # 1. The full string is not in any subdomain whitelist (in case it's a subdomain).
                        # 2. Its main domain is not in any domain whitelist.
                        if (cleaned_domain_str not in whitelist_inputs.get("WhiteListSubDomains.txt", set()) and
                            cleaned_domain_str not in whitelist_inputs.get("WhiteListSubDomainsMail.txt", set()) and
                            main_domain not in whitelist_inputs.get("WhiteListDomains.txt", set()) and
                            main_domain not in whitelist_inputs.get("WhiteListDomainsMail.txt", set())):
                            valid_domains.add(cleaned_domain_str)
                    else:
                        invalid_domains[file_name].append((domain, error_msg))
            domain_sets[file_name] = valid_domains
        else:
            print(f"Warning: {file_name} not found. Skipping.")
            domain_sets[file_name] = set()
    
    # --- Process non-whitelist subdomain files ---
    for file_name in subdomains_priority:
        file_path = input_path / file_name
        valid_subdomains = set()
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    subdomain = line.strip()
                    if not subdomain or subdomain.startswith('#'):
                        continue
                    cleaned_subdomain_str = clean_domain(subdomain)
                    valid_status, error_msg = validate_domain(cleaned_subdomain_str)
                    if valid_status:
                        # Exclude any subdomain that appears in a whitelist subdomain file.
                        # Also, if its main domain appears in a whitelist domain file, exclude it.
                        main_domain = get_main_domain(cleaned_subdomain_str, public_suffixes)
                        
                        if (cleaned_subdomain_str not in whitelist_inputs.get("WhiteListSubDomains.txt", set()) and
                            cleaned_subdomain_str not in whitelist_inputs.get("WhiteListSubDomainsMail.txt", set()) and
                            main_domain not in whitelist_inputs.get("WhiteListDomains.txt", set()) and
                            main_domain not in whitelist_inputs.get("WhiteListDomainsMail.txt", set())):
                            valid_subdomains.add(cleaned_subdomain_str)
                    else:
                        invalid_subdomains[file_name].append((subdomain, error_msg))
            subdomain_sets[file_name] = valid_subdomains
        else:
            print(f"Warning: {file_name} not found. Skipping.")
            subdomain_sets[file_name] = set()
    
    # --- Remove duplicates among regular domain files based on priority ---
    for i, high_priority_file in enumerate(files_priority):
        if high_priority_file not in domain_sets:
            continue
        for low_priority_file in files_priority[i+1:]:
            if low_priority_file not in domain_sets:
                continue
            duplicates = domain_sets[high_priority_file] & domain_sets[low_priority_file]
            domain_sets[low_priority_file] -= duplicates
    
    # --- Remove duplicates among regular subdomain files based on priority ---
    for i, high_priority_file in enumerate(subdomains_priority):
        if high_priority_file not in subdomain_sets:
            continue
        for low_priority_file in subdomains_priority[i+1:]:
            if low_priority_file not in subdomain_sets:
                continue
            duplicates = subdomain_sets[high_priority_file] & subdomain_sets[low_priority_file]
            subdomain_sets[low_priority_file] -= duplicates
    
    # --- Write processed non-whitelist domain files ---
    for file_name, domains in domain_sets.items():
        if domains:
            output_file = output_path / f"cleaned_{file_name}"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(domains)))
            print(f"Created {output_file} with {len(domains)} unique domains")
    
    # --- Write processed non-whitelist subdomain files ---
    for file_name, subdomains in subdomain_sets.items():
        if subdomains:
            output_file = output_path / f"cleaned_{file_name}"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(subdomains)))
            print(f"Created {output_file} with {len(subdomains)} unique subdomains")
    
    # --- Write whitelist domain outputs only for those files that existed in input ---
    for file_name, content in whitelist_domain_outputs.items():
        if content:
            output_file = output_path / f"cleaned_{file_name}"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(content)))
            print(f"Created {output_file} with {len(content)} whitelist domain entries")
    
    # --- Write whitelist subdomain outputs (either read or derived) only if there is content ---
    for file_name, content in derived_whitelist_subdomains.items():
        if content:
            output_file = output_path / f"cleaned_{file_name}"
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write('\n'.join(sorted(content)))
            print(f"Created {output_file} with {len(content)} whitelist subdomain entries")
    
    # --- Write invalid domains and subdomains report (if any) ---
    if invalid_domains or invalid_subdomains:
        invalid_file = output_path / "invalid_domains_and_subdomains.txt"
        with open(invalid_file, 'w', encoding='utf-8') as f:
            f.write("Invalid Domains and Subdomains Report\n")
            f.write("=====================================\n\n")
            for file_name, entries in invalid_domains.items():
                f.write(f"\n{file_name}:\n")
                for domain, error in entries:
                    f.write(f"  - {domain}: {error}\n")
            for file_name, entries in invalid_subdomains.items():
                f.write(f"\n{file_name} (subdomains):\n")
                for subdomain, error in entries:
                    f.write(f"  - {subdomain}: {error}\n")
        print(f"\nWarning: Found invalid domains and subdomains. See {invalid_file} for details")

if __name__ == "__main__":    
    parser = argparse.ArgumentParser(description='Process domain files and remove duplicates.')
    parser.add_argument('--input-dir', default=".", help='Directory containing domain files')
    parser.add_argument('--output-dir', default="cleaned", help='Directory for cleaned files')
    
    args = parser.parse_args()
    process_domain_files(args.input_dir, args.output_dir)
